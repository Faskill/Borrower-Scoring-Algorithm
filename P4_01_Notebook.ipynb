{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc887fa",
   "metadata": {},
   "source": [
    "# Developping a Borrower Scoring Algorithm\n",
    "\n",
    "Last updated : September 25th, 2022\n",
    "\n",
    "## Introduction\n",
    "\n",
    "During this project, I will use a dataset provided by a consumer finance companies to develop a machine learning algorithm that will predict if the borrower will have payment difficulties or not.\n",
    "\n",
    "## 1. Data Loading and Filtering\n",
    "\n",
    "First we will load the necessary packages and dataset and then we will carry on with the Cleaning and Analysis.\n",
    "\n",
    "### 1.1 Loading our packages\n",
    "\n",
    "We will import the necessary packages to run this project: matplotlib, numpy, pandas, seaborn.\n",
    "Since I am running the project on Windows, I will also use sklearnex to increase the speed of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e59e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "#Importing packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#Setting large figure size for Seaborn\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27),\"font.size\":20,\"axes.titlesize\":20,\"axes.labelsize\":18})\n",
    "\n",
    "#Importing Intel extension for sklearn to improve speed\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e7df6",
   "metadata": {},
   "source": [
    "### 1.2 Loading the dataset\n",
    "\n",
    "We will now load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dede469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_test = pd.read_csv(\"Data/application_test.csv\", sep=\",\")\n",
    "# app = pd.read_csv(\"Data/application_train.csv\", sep=\",\")\n",
    "\n",
    "# app.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a01bef",
   "metadata": {},
   "source": [
    "### 1.3 Feature Filtering\n",
    "\n",
    "We will begin by removing features that have more than 50% na values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9010d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing maximum number of info rows \n",
    "pd.options.display.max_info_columns = 130\n",
    "\n",
    "#Dropping rows with more than x% na values\n",
    "def drop_na_columns(df: pd.DataFrame, percent: float):\n",
    "    n = len(df)\n",
    "    cutoff = n*percent/100\n",
    "    for c in df.columns:\n",
    "        if len(df[c].dropna()) < cutoff:\n",
    "            df.drop(columns={c}, inplace=True)\n",
    "\n",
    "# #Dropping columns with less than 50% complete fields\n",
    "# drop_na_columns(app, 50)\n",
    "\n",
    "# len(app.columns)\n",
    "\n",
    "# app.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2024f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Counting the number of target vs not target variables:\n",
    "# app[\"TARGET\"].value_counts(normalize=True)\n",
    "\n",
    "# #We have a significant difference in the number of data for both cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a35290",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "We will now clean our dataset.\n",
    "\n",
    "### 2.1 Cleaning categorical variables\n",
    "\n",
    "We will begin the cleaning process by cleaning categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9354b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Looking at unique valeus of categorical variables\n",
    "# def investigate_categories(df: pd.DataFrame):\n",
    "#     for c in df.columns:\n",
    "#         if df[c].dtype == 'object':\n",
    "#             print(\"Column\",c)\n",
    "#             print(\"Unique values: {}\".format(df[c].unique()))\n",
    "#             print(\"\")\n",
    "#             print(\"-----------------------------------\")\n",
    "            \n",
    "# investigate_categories(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152ac3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Investigating \"XNA\" values in GENDER\n",
    "# app[app[\"CODE_GENDER\"] == 'XNA']\n",
    "# #Only 4 rows\n",
    "\n",
    "# #Let's look at the test data\n",
    "# app_test[app_test[\"CODE_GENDER\"] == 'XNA']\n",
    "# #0 row\n",
    "\n",
    "# #We will replace with the mode\n",
    "# app[\"CODE_GENDER\"] = app[\"CODE_GENDER\"].fillna(app[\"CODE_GENDER\"].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8ca7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Investigating \"XNA\" values in ORGANIZATION_TYPE\n",
    "# app[app[\"ORGANIZATION_TYPE\"] == 'XNA']\n",
    "# #55374 rows\n",
    "\n",
    "# app[app[\"ORGANIZATION_TYPE\"] == 'XNA'][\"TARGET\"].value_counts(normalize=True)\n",
    "# #Significant deviation from the normal percentages, so it is interesting to keep these values\n",
    "\n",
    "# #They will be encoded during the feature engineering part of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ec0994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Looking at \"nan\" values in EMERGENCYSTATE_MODE\n",
    "# print(len(app[app[\"EMERGENCYSTATE_MODE\"].isna()]))\n",
    "\n",
    "# app[app[\"EMERGENCYSTATE_MODE\"].isna()][\"TARGET\"].value_counts(normalize=True)\n",
    "# #Here it represents about half our dataset, we will create a \"NA\" variable as well since there is a small deviation from what\n",
    "# #We would have expected\n",
    "\n",
    "# app.loc[app[\"EMERGENCYSTATE_MODE\"].isna(),\"EMERGENCYSTATE_MODE\"] = 'UKN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602bee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Looking at \"nan\" values in OCCUPATION TYPE\n",
    "# print(len(app[app[\"OCCUPATION_TYPE\"].isna()]))\n",
    "\n",
    "# app[app[\"OCCUPATION_TYPE\"].isna()][\"TARGET\"].value_counts(normalize=True)\n",
    "# #Here it represents about a third of our dataset, we will create a \"NA\" variable as well since there is a deviation from what\n",
    "# #we would have expected\n",
    "\n",
    "# app.loc[app[\"OCCUPATION_TYPE\"].isna(),\"OCCUPATION_TYPE\"] = 'UKN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec06962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Looking at \"nan\" values in NAME_TYPE_SUITE\n",
    "# print(len(app[app[\"NAME_TYPE_SUITE\"].isna()]))\n",
    "# #Only 1292 NA values\n",
    "\n",
    "# #We will replace these rows by the mode\n",
    "# app[\"NAME_TYPE_SUITE\"] = app[\"NAME_TYPE_SUITE\"].fillna(app[\"NAMLE_TYPE_SUITE\"].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bffbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #We can see that WEEKDAY_APPR_PROCESS_START is coded as a string\n",
    "\n",
    "# import time\n",
    "# #Let's convert it into week day number\n",
    "# app[\"WEEKDAY_APPR_PROCESS_START\"] = app[\"WEEKDAY_APPR_PROCESS_START\"].apply(lambda x: time.strptime(x, '%A').tm_wday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296c309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Verifying that we've dealt with all missing values of categorical variables\n",
    "# for c in app.columns:\n",
    "#     if app[c].dtype == 'object':\n",
    "#         print(app[c].isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38c8b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#Summarizing preprocessing of categorical variables\n",
    "def preprocess_cat_vars(df: pd.DataFrame):\n",
    "\n",
    "    df[\"CODE_GENDER\"] = df[\"CODE_GENDER\"].fillna(df[\"CODE_GENDER\"].mode())\n",
    "\n",
    "    df.loc[df[\"EMERGENCYSTATE_MODE\"].isna(),\"EMERGENCYSTATE_MODE\"] = 'UKN'\n",
    "\n",
    "    df.loc[df[\"OCCUPATION_TYPE\"].isna(),\"OCCUPATION_TYPE\"] = 'UKN'\n",
    "\n",
    "    df[\"NAME_TYPE_SUITE\"] = df[\"NAME_TYPE_SUITE\"].fillna(df[\"NAME_TYPE_SUITE\"].mode())\n",
    "\n",
    "    #Let's convert it into week day number\n",
    "    df[\"WEEKDAY_APPR_PROCESS_START\"] = df[\"WEEKDAY_APPR_PROCESS_START\"].apply(lambda x: time.strptime(x, '%A').tm_wday)\n",
    "    \n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d158d",
   "metadata": {},
   "source": [
    "We have finished cleaning up categorical variables, now we will look at numeric variables \n",
    "\n",
    "### 2.2 Cleaning numeric variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae177e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking for outliers \n",
    "\n",
    "#Increasing the number of maximum columns shown\n",
    "pd.options.display.max_columns = 100\n",
    "#app.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9467e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DAYS_BIRTH, DAYS_REGISTRATION and DAYS_ID_PUBLISH only have negative values\n",
    "# app[\"DAYS_REGISTRATION\"] = abs(app[\"DAYS_REGISTRATION\"])\n",
    "# app[\"DAYS_ID_PUBLISH\"] = abs(app[\"DAYS_ID_PUBLISH\"])\n",
    "# app[\"DAYS_BIRTH\"] = abs(app[\"DAYS_BIRTH\"])\n",
    "\n",
    "# #DAYS EMPLOYED have abherrent values (365243 days, about 1000 years)\n",
    "# app.loc[app[\"DAYS_EMPLOYED\"] > 100000, \"DAYS_EMPLOYED\"] = np.nan\n",
    "# app[\"DAYS_EMPLOYED\"] = abs(app[\"DAYS_EMPLOYED\"])\n",
    "\n",
    "# print(app[\"DAYS_BIRTH\"].min()/365, app[\"DAYS_BIRTH\"].max()/365)\n",
    "# #No outlier data, from 20 to 69 years\n",
    "\n",
    "# def label_age(days_birth):\n",
    "#     age_years = days_birth / 365\n",
    "#     if age_years < 30: return 1\n",
    "#     elif age_years < 40: return 2\n",
    "#     elif age_years < 50: return 3\n",
    "#     elif age_years < 60: return 4\n",
    "#     elif age_years < 70: return 5\n",
    "#     else: return 0\n",
    "    \n",
    "# app[\"AGE_LABEL\"] = app[\"DAYS_BIRTH\"].apply(lambda x: label_age(x))\n",
    "\n",
    "# app = app[app['AMT_INCOME_TOTAL'] < 20000000] # remove an outlier (117 million)\n",
    "\n",
    "# # Calculated features\n",
    "# app['DAYS_EMPLOYED_PCT'] = app['DAYS_EMPLOYED'] / app['DAYS_BIRTH']\n",
    "# app['INCOME_CREDIT_PCT'] = app['AMT_INCOME_TOTAL'] / app['AMT_CREDIT']\n",
    "# app['INCOME_PER_PERSON'] = app['AMT_INCOME_TOTAL'] / app['CNT_FAM_MEMBERS']\n",
    "# app['ANNUITY_INCOME_PCT'] = app['AMT_ANNUITY'] / app['AMT_INCOME_TOTAL']\n",
    "# app['PAYMENT_RATE'] = app['AMT_ANNUITY'] / app['AMT_CREDIT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa11805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcc2fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Turning SK_ID_CURR into an ID field :\n",
    "# app.set_index('SK_ID_CURR', inplace=True)\n",
    "\n",
    "# app.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622e7a43",
   "metadata": {},
   "source": [
    "Analysis of the describe() output shows that there is **no clear outlier** in the rest of the numeric data. We can now start handling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e5d2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(app.columns[app.isnull().any()])\n",
    "# #21 columns with NA values\n",
    "\n",
    "# #Dropping rows with more than 50% na values\n",
    "# def drop_na_rows(df: pd.DataFrame, pct: float):\n",
    "#     n = len(df.columns)\n",
    "#     cutoff = n*pct/100 \n",
    "#     df = df[df.isna().sum(axis=1) > cutoff]\n",
    "\n",
    "# drop_na_rows(app, 50)\n",
    "#No row was removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d9c8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizing cleaning of numeric variables:\n",
    "\n",
    "def preprocess_num_vars(df: pd.DataFrame,\n",
    "                        quantiles=[25650.0,81000.0,99000.0,112500.0,135000.0,147150.0,162000.0,180000.0,225000.0,270000.0,117000000.0]):\n",
    "    #Setting index\n",
    "    data = df.copy()\n",
    "    data.set_index('SK_ID_CURR', inplace=True)\n",
    "    \n",
    "    #DAYS_BIRTH, DAYS_REGISTRATION and DAYS_ID_PUBLISH only have negative values\n",
    "    data[\"DAYS_REGISTRATION\"] = abs(data[\"DAYS_REGISTRATION\"])\n",
    "    data[\"DAYS_ID_PUBLISH\"] = abs(data[\"DAYS_ID_PUBLISH\"])\n",
    "    data[\"DAYS_BIRTH\"] = abs(data[\"DAYS_BIRTH\"])\n",
    "\n",
    "    #DAYS EMPLOYED have abherrent values (365243 days, about 1000 years)\n",
    "    data[\"DAYS_EMPLOYED_ANOM_FLAG\"] = 0 #Creating anomaly flag\n",
    "    data.loc[data[\"DAYS_EMPLOYED\"] > 100000, \"DAYS_EMPLOYED_ANOM_FLAG\"] = 1 \n",
    "    data.loc[data[\"DAYS_EMPLOYED\"] > 100000, \"DAYS_EMPLOYED\"] = np.nan\n",
    "    data[\"DAYS_EMPLOYED\"] = abs(data[\"DAYS_EMPLOYED\"])\n",
    "\n",
    "    def label_age(days_birth):\n",
    "        age_years = days_birth / 365\n",
    "        if age_years < 30: return 1\n",
    "        elif age_years < 40: return 2\n",
    "        elif age_years < 50: return 3\n",
    "        elif age_years < 60: return 4\n",
    "        elif age_years < 70: return 5\n",
    "        else: return 0\n",
    "\n",
    "    data[\"AGE_LABEL\"] = data[\"DAYS_BIRTH\"].apply(lambda x: label_age(x))\n",
    "\n",
    "    data = data[data['AMT_INCOME_TOTAL'] < 20000000] # remove an outlier (117 million)\n",
    "\n",
    "    \n",
    "    data[\"AMT_INCOME_BIN\"] = pd.cut(data[\"AMT_INCOME_TOTAL\"], bins=quantiles, labels=False)\n",
    "\n",
    "    # Calculated features\n",
    "    data['DAYS_EMPLOYED_PCT'] = data['DAYS_EMPLOYED'] / data['DAYS_BIRTH']\n",
    "    data['INCOME_CREDIT_PCT'] = data['AMT_INCOME_TOTAL'] / data['AMT_CREDIT']\n",
    "    data['INCOME_PER_PERSON'] = data['AMT_INCOME_TOTAL'] / data['CNT_FAM_MEMBERS']\n",
    "    data['ANNUITY_INCOME_PCT'] = data['AMT_ANNUITY'] / data['AMT_INCOME_TOTAL']\n",
    "    data['PAYMENT_RATE'] = data['AMT_ANNUITY'] / data['AMT_CREDIT']\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4602298",
   "metadata": {},
   "source": [
    "We've now finished cleaning incorrect values. \n",
    "Before starting to perform data imputation, we need to perform a **train/validation/test split**. This will **prevent us from introducing data leakage during the cleaning process**. \n",
    "\n",
    "### 2.3 Performing train / test / validation split\n",
    "\n",
    "We will divide our dataset as such : \n",
    "\n",
    "-  80% train set \n",
    "-  10% validation \n",
    "-  10% test\n",
    "\n",
    "We will be able to revisit this values during the hyperparameter tuning part of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d6c6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def perform_split(df: pd.DataFrame, target_col: str, validation_set: bool, shuffle: bool, random_state: int, test_size: float):\n",
    "\n",
    "    y = df[target_col]\n",
    "    ID = df.index\n",
    "    X = df.drop(columns={target_col})\n",
    "\n",
    "    #Splitting train and test sets, we have to add indices to conserve the original index\n",
    "    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(\n",
    "        X, y, ID, test_size=test_size, stratify=y, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    #Assigning the correct indices (the SK_IDs) to y_test\n",
    "    y_test.index = indices_test\n",
    "    \n",
    "    if validation_set:\n",
    "        #Applying the same function to separate train and validation set\n",
    "        X_train, X_val, y_train, y_val, indices_train, indices_val = train_test_split(\n",
    "            X_train, y_train, indices_train, test_size = test_size/(1-test_size), stratify=y, \n",
    "            shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "        #Assigning the SK IDs to y_train and y_val\n",
    "        y_val.index = indices_val\n",
    "        return X_train, y_train, X_test, y_test, X_val, y_val\n",
    "    \n",
    "    y_train.index = indices_train\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d91f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(X_train), len(X_test), len(X_val))\n",
    "#Our test and validation set have the same length and its 10% of the overall length of X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350f09b",
   "metadata": {},
   "source": [
    "Now that we have performed the split, we can carry on to perform data imputation.\n",
    "\n",
    "These operations will also have to be performed on the test and train_set, so we will create a function that we will be able to apply to the 3 sets.\n",
    "\n",
    "### 2.4 Data Imputation\n",
    "\n",
    "First we will investigate what columns still have missing values. \n",
    "Normally, we have replaced all missing features for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dbb1604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For ease of use, we will rename X_train to df so we can better replicate our code afterwards\n",
    "# df = X_train.copy()\n",
    "\n",
    "# def check_col_nas_type(df: pd.DataFrame):\n",
    "#     type_cols = []\n",
    "#     #Verifying the type of columns with missing values\n",
    "#     for c in df.columns[df.isna().any()].tolist():\n",
    "#         if ~np.isin(df[c].dtype, type_cols):\n",
    "#             type_cols.append(df[c].dtype)\n",
    "#     return(type_cols)\n",
    "\n",
    "# check_col_nas_type(df)\n",
    "# #This verifies that we only need to perform data imputation on numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70bc8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Loading visualization functions present in the functions.py file\n",
    "# from functions import *\n",
    "\n",
    "# #Visualizing distribution of all numeric variables\n",
    "# histPlotAll(df)\n",
    "\n",
    "# #Apart from HOUR_APPR_PROCESS_START, all numeric variables seem to be not normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48b97081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "#Defining a data imputation function, we will use the NAME_CONTRACT_TYPE as a category_column\n",
    "\n",
    "#This data_imputation script can be improved during the hyperparameter setting phase\n",
    "\n",
    "def numeric_data_imputation(df: pd.DataFrame, max_unique_values=3, use_categ_column=False, category_column='NAME_CONTRACT_TYPE'):\n",
    "    \n",
    "    #Creating a copy of our dataset\n",
    "    df_imput = df.copy()\n",
    "    #Creating a list of columns with missing values\n",
    "    missing_cols = df.columns[df.isna().any()].tolist()\n",
    "    \n",
    "    #Iterating over columns with missing data\n",
    "    for c in missing_cols:\n",
    "        \n",
    "        #Verifying that we are in a numeric column\n",
    "        if np.issubdtype(df[c].dtype,np.number):\n",
    "            \n",
    "            #If there are less or equal to max unique values, we will use mode imputation \n",
    "            if len(df[c].unique()) <= max_unique_values:\n",
    "                \n",
    "                if use_categ_column:\n",
    "                            \n",
    "                    #We will create a subset from our categorical variable and perform mode imputation\n",
    "                    for t in df[category_column].unique():\n",
    "                        #Creating subset\n",
    "                        subset = df.loc[df[category_column] == t]\n",
    "\n",
    "                        #Calculating mode of subset\n",
    "                        mode = subset[c].mode()\n",
    "\n",
    "                        #Applying imputation\n",
    "                        df.loc[(df[c].isna()) & (df[category_column] == t), c] = mode\n",
    "                \n",
    "                else:\n",
    "                    df[c] = df[c].fillna(df[c].mode())\n",
    "                            \n",
    "            #If we have more numeric values, we will calculate the Kolmogorov Smirnoff pvalue to test for normalization\n",
    "            else:\n",
    "                \n",
    "                #Normalizing target variable\n",
    "                norm = c + '_norm'\n",
    "                df_imput[norm] = (df_imput[c] - np.mean(df_imput[c].dropna())) / np.std(df_imput[c].dropna())\n",
    "\n",
    "                #Calculating pvalue of KS test\n",
    "                pval = stats.kstest(df_imput[norm].dropna(), 'norm').pvalue\n",
    "                \n",
    "                if pval >= 0.05:\n",
    "                #P value is superior to 0.05, we cannot reject the null hypothesis and thus conclude the variable is\n",
    "                #approximatively normally distributed\n",
    "                #We will use mean imputation on that variable                    \n",
    "                    if use_categ_column:\n",
    "\n",
    "                        for t in df[category_column].unique():\n",
    "                            #Creating subset\n",
    "                            subset = df.loc[df[category_column] == t]\n",
    "\n",
    "                            #Calculating mean based on that subset and our target column\n",
    "                            mean = subset[c].mean()\n",
    "\n",
    "                            #Applying imputation\n",
    "                            df.loc[(df[c].isna()) & (df[category_column] == t), c] = mean\n",
    "\n",
    "                    else:\n",
    "                        df[c] = df[c].fillna(df[c].mean())\n",
    "                            \n",
    "                else:\n",
    "                    \n",
    "                    if use_categ_column:\n",
    "                        \n",
    "                        #P value is inferior to 0.05, we can reject the null hypothesis and thus conclude the variable is\n",
    "                        #not normally distributed\n",
    "                        #We will use median imputation on that variable\n",
    "                        for t in df[category_column].unique():\n",
    "                            #Creating subset\n",
    "                            subset = df.loc[df[category_column] == t]\n",
    "\n",
    "                            #Calculating mean based on that subset and our target column\n",
    "                            med = subset[c].median()\n",
    "\n",
    "                            #Applying imputation\n",
    "                            df.loc[(df[c].isna()) & (df[category_column] == t), c] = med\n",
    "                            \n",
    "                    else:\n",
    "                        df[c] = df[c].fillna(df[c].median())\n",
    "    return None\n",
    "\n",
    "# #Applying the function to our 3 sets (X_train has been renamed to df)\n",
    "# numeric_data_imputation(df, 'NAME_CONTRACT_TYPE')\n",
    "# numeric_data_imputation(X_test, 'NAME_CONTRACT_TYPE')\n",
    "# numeric_data_imputation(X_val, 'NAME_CONTRACT_TYPE')\n",
    "\n",
    "#Checking for nulls in our 3 sets\n",
    "# for data in [df,X_test,X_val]:\n",
    "#     print(np.count_nonzero(data.isnull()))\n",
    "    \n",
    "#We have no more NA values in all 3 sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5509975",
   "metadata": {},
   "source": [
    "Now that we have 3 complete datasets, we can perform **feature engineering**\n",
    "\n",
    "## 3. Feature Engineering\n",
    "\n",
    "We will begin by encoding cyclical features.\n",
    "\n",
    "### 3.1 Encoding Cyclical Features\n",
    "\n",
    "We have 2 columns with time features that are cyclical in nature but coded with numbers.\n",
    "\n",
    "- WEEDKAY_APPR_PROCESS_START\n",
    "- HOUR_APPR_PROCESS_START\n",
    "\n",
    "To increase the performance of our algorithm, we will apply a cyclical encoding algorithm to better represent their cyclical nature :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "583ae756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cyclical_vars(df: pd.DataFrame, cyclical_vars=[\"WEEKDAY_APPR_PROCESS_START\", \"HOUR_APPR_PROCESS_START\"]):\n",
    "    for c in cyclical_vars:\n",
    "        #Calculating the number of unique values\n",
    "        n = len(df[c].unique())\n",
    "        #Defining variable names\n",
    "        cos_var = c + '_cos'\n",
    "        sin_var = c + '_sin'\n",
    "        #Calculating cyclical encoder variables\n",
    "        df[sin_var] = np.sin(df[c] * (2*np.pi/n))\n",
    "        df[cos_var] = np.cos(df[c] * (2*np.pi/n))\n",
    "        #Dropping the base columns\n",
    "        df.drop(columns = {c}, inplace=True)\n",
    "\n",
    "# encode_cyclical_vars(df)\n",
    "# encode_cyclical_vars(X_test)\n",
    "# encode_cyclical_vars(X_val)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab7ff6",
   "metadata": {},
   "source": [
    "### 3.2 Encoding categorical variables\n",
    "\n",
    "Since our algorithms are only able to use numeric variables, we will need to **encode categorical variables**.\n",
    "\n",
    "For variables with a small number of categories, we will perform **One-Hot Encoding**.\n",
    "\n",
    "If there are more than 10 categories, we will perform **Weight of Evidence (WoE) encoding** instead to avoid a sharp increase in the dimensionality of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6a274c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import WOEEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "\n",
    "\n",
    "def encode_cat_vars(df: pd.DataFrame, X_train: pd.DataFrame, y_train, max_categ: int,\n",
    "                    drop_invariant=False, woe_encode=True, ordinal_encode=True):\n",
    "    if ordinal_encode:\n",
    "        ord_enc = OrdinalEncoder(cols= label_cols, drop_invariant=drop_invariant, return_df=True).fit(X_train)\n",
    "        X_train_encoded = ord_enc.transform(X_train)\n",
    "        df = ord_enc.transform(df)\n",
    "        return df\n",
    "    \n",
    "    woe_cols = []\n",
    "    ohe_cols = []\n",
    "    label_cols = []\n",
    "    for c in X_train.columns:\n",
    "        \n",
    "        #Keeping only categorical columns\n",
    "        if not np.issubdtype(X_train[c].dtype,np.number):\n",
    "            \n",
    "            #If only 2 categories, performing Label encoding\n",
    "            if len(X_train[c].unique()) == 2:\n",
    "                label_cols.append(c)\n",
    "            \n",
    "            #If more than X categories, performing WOE encoding\n",
    "            elif len(X_train[c].unique()) >= max_categ:\n",
    "                woe_cols.append(c)\n",
    "            \n",
    "            else: \n",
    "                #One hot encoding and remove the original column\n",
    "                ohe_cols.append(c)\n",
    "                \n",
    "    #Defining Binary Encoder based on the train dataset and applying it to df\n",
    "    bin_enc = BinaryEncoder(cols= label_cols, drop_invariant=drop_invariant, return_df=True).fit(X_train)\n",
    "    X_train_encoded = bin_enc.transform(X_train)\n",
    "    df = bin_enc.transform(df)\n",
    "    \n",
    "    if woe_encode:\n",
    "        #Defining WOE Encoder and fitting it to the TRAIN dataset\n",
    "        woe_encoder = WOEEncoder(cols = woe_cols, drop_invariant=drop_invariant, return_df=True).fit(X_train_encoded, y_train)\n",
    "        X_train_encoded = woe_encoder.transform(X_train_encoded)\n",
    "        #Fitting the encoder to the selected dataframe\n",
    "        df = woe_encoder.transform(df)\n",
    "    else: #Perform label (ordinal) encoding\n",
    "        label_encoder = LabelEncoder(cols=woe_cols, drop_invariant=drop_invariant, return_df=True).fit(X_train_encoded)\n",
    "        X_train_encoded = label_encoder.transform(X_train_encoded)\n",
    "        df = label_encoder.transform(df)\n",
    "    \n",
    "    #Performing one hot encoding on selected columns\n",
    "    ohe_encoder = OneHotEncoder(cols=ohe_cols, return_df= True, drop_invariant=drop_invariant).fit(X_train_encoded)\n",
    "    df = ohe_encoder.transform(df)\n",
    "    \n",
    "    \n",
    "    del X_train_encoded\n",
    "    return df\n",
    "\n",
    "#Just a reminder that once again df = X_train\n",
    "#We apply all this function to our 3 sets\n",
    "# X_test = encode_cat_vars(X_test, df, y_train, 10)\n",
    "# X_val = encode_cat_vars(X_val, df, y_train, 10)\n",
    "# df = encode_cat_vars(df, df, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3b92315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_dtypes(df: pd.DataFrame):\n",
    "#     type_cols = []\n",
    "#     for c in df.columns:\n",
    "#         if not np.isin(df[c].dtype, type_cols):\n",
    "#             type_cols.append(df[c].dtype)\n",
    "#     print(type_cols)\n",
    "\n",
    "# check_dtypes(df)\n",
    "# check_dtypes(X_test)\n",
    "# check_dtypes(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8d47e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.shape, X_test.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf2193",
   "metadata": {},
   "source": [
    "We have verified that all of our 3 sets are composed only of numeric features and that they have the same number of columns.\n",
    "\n",
    "We will now use **additional features from other dataframes** to increase the performance of our models.\n",
    "\n",
    "### 3.3 Using previous application data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10bb0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_app = pd.read_csv(\"Data/previous_application.csv\", sep=\",\")\n",
    "\n",
    "# prev_app.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1df49509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_app.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7722b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #We are interested in DAYS_LAST_DUE (the number of days the borrower has to pay previous applications)\n",
    "# #But there are illogical values (365243 which is equal to 1000 years)\n",
    "# #First we'll replace all the values by nan\n",
    "# prev_app.loc[prev_app.DAYS_LAST_DUE > 300000, \"DAYS_LAST_DUE\"] = np.nan\n",
    "# prev_app.loc[prev_app.DAYS_FIRST_DUE > 300000, \"DAYS_FIRST_DUE\"] = np.nan\n",
    "# prev_app.loc[prev_app.DAYS_LAST_DUE_1ST_VERSION > 300000, \"DAYS_LAST_DUE_1ST_VERSION\"] = np.nan\n",
    "# prev_app.loc[prev_app.DAYS_FIRST_DRAWING > 300000, \"DAYS_FIRST_DRAWING\"] = np.nan\n",
    "# prev_app.loc[prev_app.DAYS_TERMINATION > 300000, \"DAYS_TERMINATION\"] = np.nan\n",
    "\n",
    "# #Defining current amount due, we have to add a negative sign because DAYS_LAST_DUE is negative\n",
    "# prev_app[\"AMT_CURR_DUE\"] = -prev_app[\"AMT_ANNUITY\"]*prev_app[\"DAYS_LAST_DUE\"]/365\n",
    "\n",
    "# prev_app[\"CURR_ANNUITY\"] = 0\n",
    "# prev_app.loc[prev_app[\"DAYS_LAST_DUE\"] < 0, \"CURR_ANNUITY\"] = prev_app[\"AMT_ANNUITY\"]\n",
    "\n",
    "# # Calculated variables\n",
    "# prev_app['APPLICATION_CREDIT_DIF'] = prev_app['AMT_APPLICATION'] - prev_app['AMT_CREDIT']\n",
    "# prev_app['CREDIT_TO_ANNUITY'] = prev_app['AMT_CREDIT'] / prev_app['AMT_ANNUITY']\n",
    "# prev_app['DOWN_PAYMENT_TO_CREDIT'] = prev_app['AMT_DOWN_PAYMENT'] / prev_app['AMT_CREDIT']\n",
    "\n",
    "# prev_app.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "688c8709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Verifying unique values of contract status\n",
    "# prev_app.NAME_CONTRACT_STATUS.unique()\n",
    "# #4 categories, Approved, Refused, Canceled and Unused offer\n",
    "\n",
    "# prev_app[\"AMT_GRANTED\"] = 0\n",
    "# prev_app.loc[prev_app[\"NAME_CONTRACT_STATUS\"] == \"Approved\", \"AMT_GRANTED\"] = prev_app[\"AMT_CREDIT\"]\n",
    "\n",
    "# prev_app.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8326702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregations = {\n",
    "#         'AMT_ANNUITY': ['std', 'mean', 'sum'],\n",
    "#         'AMT_APPLICATION': ['std', 'mean', 'sum'],\n",
    "#         'AMT_CREDIT': ['std', 'mean', 'sum'],\n",
    "#         'AMT_CURR_DUE': ['std', 'mean', 'sum'],\n",
    "#         'CURR_ANNUITY': ['std', 'mean', 'sum'],\n",
    "#         'AMT_DOWN_PAYMENT': ['std', 'mean', 'sum'],\n",
    "#         'AMT_GOODS_PRICE': ['std', 'mean', 'sum'],\n",
    "#         'HOUR_APPR_PROCESS_START': ['std', 'mean'],\n",
    "#         'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "#         'DAYS_DECISION': ['std', 'mean', 'sum'],\n",
    "#         'CNT_PAYMENT': ['mean', 'sum','std'],\n",
    "#         'SK_ID_PREV': ['nunique'],\n",
    "#         'DAYS_TERMINATION': ['mean', 'sum', 'std'],\n",
    "#         'DOWN_PAYMENT_TO_CREDIT': ['sum', 'mean', 'std']\n",
    "#     }\n",
    "\n",
    "# #We will aggregate by SK_ID_CURR and retrieve important information about previous applications :\n",
    "# prev_app_numbers = prev_app.groupby(\"SK_ID_CURR\").agg(aggregations).fillna(0)\n",
    "\n",
    "# #Renaming columns to remove multi indexing\n",
    "# prev_app_numbers.columns = pd.Index(['APP' + '_' + e[0] + '_' + e[1] for e in prev_app_numbers.columns])\n",
    "\n",
    "# prev_app_numbers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0969f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating a dataframe with the number of each different name contract status by SK_ID_CURR\n",
    "# prev_app_status = pd.crosstab(prev_app['SK_ID_CURR'], prev_app['NAME_CONTRACT_STATUS'])\n",
    "\n",
    "# cols = [\"N_PREV_APPROVED\",\"N_PREV_CANCELED\",\"N_PREV_REFUSED\",\"N_PREV_UNUSED\"]\n",
    "# prev_app_status.columns = cols\n",
    "\n",
    "# #Importing the number of unique applications from prev_app_numbers\n",
    "# prev_app_status = pd.merge(prev_app_status, prev_app_numbers[[\"APP_SK_ID_PREV_nunique\"]],\n",
    "#                            how=\"inner\", left_index=True, right_index=True)\n",
    "\n",
    "# for c in cols:\n",
    "#     prev_app_status[c] = prev_app_status[c] / prev_app_status[\"APP_SK_ID_PREV_nunique\"]\n",
    "\n",
    "# prev_app_status = prev_app_status.drop(columns={\"APP_SK_ID_PREV_nunique\"})\n",
    "# prev_app_status.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "221260cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_app_df = pd.merge(prev_app_numbers, prev_app_status, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "# prev_app_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df098c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Saving prev_app_df to prevent RAM usage and reduce rerun time\n",
    "# prev_app_df.to_csv(\"Data/prev_app_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bcbab37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prev_app_info(X_train, X_test, X_val=None, validation_set=False):\n",
    "\n",
    "    #Joining this new data and filling NAs with 0 (since it means there was no previous application)\n",
    "    X_train = pd.merge(X_train, prev_app_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "    X_test = pd.merge(X_test, prev_app_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "    if validation_set:\n",
    "        X_val = pd.merge(X_val, prev_app_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "        return X_train, X_test, X_val\n",
    "    return X_train, X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ebb22",
   "metadata": {},
   "source": [
    "### 3.4 Using Credit Bureau information\n",
    "\n",
    "We also have information about CB for each borrower that we can use to increase the accuracy of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99e28b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1716428 entries, 0 to 1716427\n",
      "Data columns (total 17 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   SK_ID_CURR              int64  \n",
      " 1   SK_ID_BUREAU            int64  \n",
      " 2   CREDIT_ACTIVE           object \n",
      " 3   CREDIT_CURRENCY         object \n",
      " 4   DAYS_CREDIT             int64  \n",
      " 5   CREDIT_DAY_OVERDUE      int64  \n",
      " 6   DAYS_CREDIT_ENDDATE     float64\n",
      " 7   DAYS_ENDDATE_FACT       float64\n",
      " 8   AMT_CREDIT_MAX_OVERDUE  float64\n",
      " 9   CNT_CREDIT_PROLONG      int64  \n",
      " 10  AMT_CREDIT_SUM          float64\n",
      " 11  AMT_CREDIT_SUM_DEBT     float64\n",
      " 12  AMT_CREDIT_SUM_LIMIT    float64\n",
      " 13  AMT_CREDIT_SUM_OVERDUE  float64\n",
      " 14  CREDIT_TYPE             object \n",
      " 15  DAYS_CREDIT_UPDATE      int64  \n",
      " 16  AMT_ANNUITY             float64\n",
      "dtypes: float64(8), int64(6), object(3)\n",
      "memory usage: 222.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# bureau = pd.read_csv(\"Data/bureau.csv\", sep=\",\")\n",
    "\n",
    "# bureau.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc29681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bureau.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "724001f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bureau.CREDIT_ACTIVE.unique())\n",
    "# print(bureau.CREDIT_CURRENCY.unique())\n",
    "\n",
    "# len(bureau[bureau.CREDIT_CURRENCY.isna()])\n",
    "# #Credit active is interesting because of the bad debt field\n",
    "# #Currency is also interesting because it could be an indicator to fraudulent transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5478dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Calculating new features\n",
    "\n",
    "# #Date differences\n",
    "# bureau['CREDIT_DURATION'] = -bureau['DAYS_CREDIT'] + bureau['DAYS_CREDIT_ENDDATE']\n",
    "# bureau['ENDDATE_DIF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
    "\n",
    "# #Day overdue flags:\n",
    "# bureau['BUREAU_IS_DPD'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n",
    "# bureau['BUREAU_IS_DPD_OVER100'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 100 else 0)\n",
    "                                                                     \n",
    "# #Debt ratio                                                         \n",
    "# bureau['BUREAU_CREDIT_DEBT_RATIO'] = bureau['AMT_CREDIT_SUM_DEBT'] / bureau['AMT_CREDIT_SUM']\n",
    "\n",
    "\n",
    "# #We will now aggreagte over SK_ID_CURR to calculate relevant numeric features \n",
    "\n",
    "# aggregations = {\n",
    "#         'DAYS_CREDIT': ['sum', 'mean', 'std'],\n",
    "#         'DAYS_CREDIT_ENDDATE': ['sum', 'mean', 'std'],\n",
    "#         'CREDIT_DAY_OVERDUE': ['min', 'max', 'sum','mean', 'std'],\n",
    "#         'AMT_CREDIT_MAX_OVERDUE': ['min', 'max', 'sum', 'mean', 'std'],\n",
    "#         'AMT_CREDIT_SUM': ['min', 'max', 'sum', 'mean', 'std'],\n",
    "#         'AMT_CREDIT_SUM_DEBT': ['min', 'max', 'sum', 'mean', 'std'],\n",
    "#         'AMT_CREDIT_SUM_OVERDUE': ['min', 'max', 'sum', 'mean', 'std'],\n",
    "#         'AMT_CREDIT_SUM_LIMIT': ['min', 'max', 'sum', 'mean', 'std'],\n",
    "#         'AMT_ANNUITY': ['min', 'max', 'mean', 'sum', 'std'],\n",
    "#         'CNT_CREDIT_PROLONG': ['sum'],\n",
    "#         'SK_ID_BUREAU': ['count'],\n",
    "#         'DAYS_ENDDATE_FACT': ['min', 'max', 'mean', 'std'],\n",
    "#         'ENDDATE_DIF': ['min', 'max', 'mean', 'std'],\n",
    "#         'BUREAU_CREDIT_DEBT_RATIO': ['min', 'max', 'mean','std'],\n",
    "#         'BUREAU_IS_DPD': ['mean', 'sum', 'std'],\n",
    "#         'BUREAU_IS_DPD_OVER100': ['mean', 'sum', 'std']\n",
    "# } \n",
    "                                                                     \n",
    "# bureau_num = bureau.groupby(\"SK_ID_CURR\").agg(aggregations).fillna(0)\n",
    "\n",
    "# #Renaming columns to remove multi indexing\n",
    "# bureau_num.columns = pd.Index(['BUREAU' + '_' + e[0] + '_' + e[1] for e in bureau_num.columns])\n",
    "\n",
    "# bureau_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67e88fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #We will count the number of CB credits with each of these attributes :\n",
    "# bureau_categ1 = pd.crosstab(bureau['SK_ID_CURR'], bureau['CREDIT_ACTIVE'])\n",
    "# bureau_categ2 = pd.crosstab(bureau['SK_ID_CURR'], bureau['CREDIT_CURRENCY'])\n",
    "\n",
    "# bureau_categ = pd.merge(bureau_categ1, bureau_categ2, how=\"outer\", left_index=True, right_index=True)\n",
    "\n",
    "# cols = ['CB_ACTIVE', 'CB_BAD_DEBT', 'CB_CLOSED', 'CB_SOLD',\n",
    "#                         'CB_CURR1', 'CB_CURR2', 'CB_CURR3', 'CB_CURR4']\n",
    "# bureau_categ.columns = cols\n",
    "\n",
    "# bureau_categ = pd.merge(bureau_categ, bureau_num[[\"BUREAU_SK_ID_BUREAU_count\"]], how=\"inner\", left_index=True, right_index=True)\n",
    "\n",
    "# for c in cols:\n",
    "#     bureau_categ[c] = bureau_categ[c] / bureau_categ[\"BUREAU_SK_ID_BUREAU_count\"]\n",
    "# bureau_categ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6015ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #We now load the bureau_balance csv file\n",
    "# bureau_balance = pd.read_csv(\"Data/bureau_balance.csv\", sep=',')\n",
    "\n",
    "# bureau_balance.STATUS.value_counts(normalize=True)\n",
    "\n",
    "# bureau_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95d7f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #We create a crosstab to count the number of status type for each sk_id_bureau\n",
    "# bureau_balance_stats = pd.crosstab(bureau_balance['SK_ID_BUREAU'], bureau_balance['STATUS'])\n",
    "\n",
    "# bureau_balance_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1caadbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Counting the number of columns for each sk_id_bureau\n",
    "# bureau_balance_count = bureau_balance[[\"SK_ID_BUREAU\",\"MONTHS_BALANCE\"]].groupby(\"SK_ID_BUREAU\").count()\n",
    "\n",
    "# bureau_balance_count.columns = [\"CB_COUNT\"]\n",
    "\n",
    "# #Renaming the columns for better clarity\n",
    "# cols = [\"CB_DPD_0\",\"CB_DPD_1\",\"CB_DPD_2\",\"CB_DPD_3\",\"CB_DPD_4\",\"CB_DPD_5\",\"CB_BAL_CLOSED\",\"CB_BAL_UKN\"]\n",
    "# bureau_balance_stats.columns=[\"CB_DPD_0\",\"CB_DPD_1\",\"CB_DPD_2\",\"CB_DPD_3\",\"CB_DPD_4\",\"CB_DPD_5\",\"CB_BAL_CLOSED\",\"CB_BAL_UKN\"]\n",
    "\n",
    "# bureau_balance_stats = pd.merge(bureau_balance_stats, bureau_balance_count, how=\"inner\", left_index=True, right_index=True)\n",
    "\n",
    "# for c in cols:\n",
    "#     bureau_balance_stats[c] = bureau_balance_stats[c] / bureau_balance_stats[\"CB_COUNT\"]\n",
    "\n",
    "# bureau_balance_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "179f2a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Joining with the main CB dataframe to retrieve SK_ID_CURR info\n",
    "# bureau_num_bal = pd.merge(bureau_balance_stats, bureau[[\"SK_ID_BUREAU\",\"SK_ID_CURR\"]], how='inner', left_index=True, right_on='SK_ID_BUREAU')\n",
    "\n",
    "# #Creating aggregator\n",
    "# agg_functions = ['min', 'max', 'mean', 'std', 'sum']\n",
    "\n",
    "# aggregations = {\n",
    "#     c: agg_functions for c in bureau_balance_stats.columns\n",
    "# }\n",
    "\n",
    "# #Aggregating by SK_ID_CURR\n",
    "# bureau_num_bal = bureau_num_bal.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "\n",
    "# #Renaming columns to remove multi indexing\n",
    "# bureau_num_bal.columns = pd.Index(['BB' + '_' + e[0] + '_' + e[1] for e in bureau_num_bal.columns])\n",
    "\n",
    "# bureau_num_bal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7286d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bureau_num_bal.info()\n",
    "# #We only have 134k different SK_ID, which is about 40% of our dataset. \n",
    "# #We will fill nulls with 0 because it means that the other SK_ID were not referenced at the Credit Bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a10654e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Filling nulls with 0 as mentionned previously\n",
    "# bureau_num_full = pd.merge(bureau_num, bureau_num_bal, how='outer', left_index=True, right_index=True).fillna(0)\n",
    "\n",
    "# bureau_num_full.info()\n",
    "# bureau_num_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4824cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Merging the 2 dataframes with bureau information\n",
    "# bureau_df = pd.merge(bureau_categ, bureau_num_full, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# bureau_df.replace([np.inf, -np.inf], 0, inplace=True) #Removing infinite values\n",
    "\n",
    "# bureau_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6798683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Saving bureau_df to reduce RAM usage\n",
    "# bureau_df.to_csv(\"Data/bureau_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "801412f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bureau_info(X_train, X_test, X_val=None, validation_set=False):\n",
    "\n",
    "\n",
    "    #Joining this new data and filling NAs with 0\n",
    "    X_train = pd.merge(X_train, bureau_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "    X_test = pd.merge(X_test, bureau_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "    if validation_set:\n",
    "        X_val = pd.merge(X_val, bureau_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "        return X_train, X_test, X_val\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d3913",
   "metadata": {},
   "source": [
    "### 3.5 Using Cash balance information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87d60842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cash = pd.read_csv(\"Data/POS_CASH_balance.csv\", sep=',')\n",
    "\n",
    "# cash.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea767dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating late payment flag\n",
    "# cash['LATE_PAYMENT'] = cash['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "# cash['POS_IS_DPD_OVER_100'] = cash['SK_DPD'].apply(lambda x: 1 if x >= 100 else 0)\n",
    "\n",
    "# #Creating aggregator\n",
    "# agg_functions = ['min', 'max', 'mean', 'std', 'sum']\n",
    "\n",
    "# #Aggregating on all columns except months_balance and sk_id_prev\n",
    "# aggregations = {\n",
    "#     c: agg_functions for c in cash.drop(columns={\"SK_ID_CURR\",\"SK_ID_PREV\",\"MONTHS_BALANCE\", \"NAME_CONTRACT_STATUS\"}).columns\n",
    "# }\n",
    "\n",
    "# #Adding a nunique count on SK_ID_PREV\n",
    "# aggregations[\"SK_ID_PREV\"] = \"nunique\"\n",
    "\n",
    "# #Aggregating over \"SK_ID_CURR\"\n",
    "# cash_df = cash.groupby(\"SK_ID_CURR\").agg(aggregations).fillna(0)\n",
    "\n",
    "# #Renaming columns to remove multi indexing\n",
    "# cash_df.columns = pd.Index(['CASH' + '_' + e[0] + '_' + e[1] for e in cash_df.columns])\n",
    "\n",
    "# cash_df.replace([np.inf, -np.inf], 0, inplace=True) #Removing infinite values\n",
    "# cash_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b49861c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Saving cash_df to csv to save RAM usage\n",
    "# cash_df.to_csv(\"Data/cash_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "29f62ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cash_info(X_train, X_test, X_val=None, validation_set=False):\n",
    "\n",
    "    cash_df.set_index(\"SK_ID_CURR\", inplace=True)\n",
    "\n",
    "    #Joining this new data and filling NAs with 0\n",
    "    X_train = pd.merge(X_train, cash_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "    X_test = pd.merge(X_test, cash_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "    if validation_set:\n",
    "        X_val = pd.merge(X_val, cash_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "        return X_train, X_test, X_val\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4ab2a",
   "metadata": {},
   "source": [
    "### 3.6 Using CC Balance information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "789d73cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc = pd.read_csv(\"Data/credit_card_balance.csv\",sep=\",\")\n",
    "\n",
    "# cc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c86a005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Calculating new features\n",
    "# cc['LATE_PAYMENT'] = cc['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "# cc['CARD_IS_DPD_OVER_100'] = cc['SK_DPD'].apply(lambda x: 1 if x >= 100 else 0)\n",
    "\n",
    "# #Creating aggregator\n",
    "# agg_functions = ['min', 'max', 'mean', 'std', 'sum']\n",
    "\n",
    "# #Aggregating on all columns except months_balance and sk_id_prev\n",
    "# aggregations = {\n",
    "#     c: agg_functions for c in cc.drop(columns={\"SK_ID_CURR\",\"SK_ID_PREV\",\"MONTHS_BALANCE\", \"NAME_CONTRACT_STATUS\"}).columns\n",
    "# }\n",
    "\n",
    "# #Adding a nunique count on SK_ID_PREV\n",
    "# aggregations[\"SK_ID_PREV\"] = \"nunique\"\n",
    "\n",
    "# #Aggregating over \"SK_ID_CURR\"\n",
    "# cc_df = cc.groupby(\"SK_ID_CURR\").agg(aggregations).fillna(0)\n",
    "\n",
    "# #Renaming columns to remove multi indexing\n",
    "# cc_df.columns = pd.Index(['CC_BAL' + '_' + e[0] + '_' + e[1] for e in cc_df.columns])\n",
    "\n",
    "# cc_df.replace([np.inf, -np.inf], 0, inplace=True) #Removing infinite values\n",
    "\n",
    "# cc_df.head()\n",
    "\n",
    "# # #Investigating possible months balance values\n",
    "# # cc.MONTHS_BALANCE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb3f47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Saving cc_df to prevent high RAM usage\n",
    "# cc_df.to_csv(\"Data/cc_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "30d10fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cc_info(X_train, X_test, X_val=None, validation_set=False):\n",
    "\n",
    "    cc_df.set_index(\"SK_ID_CURR\", inplace=True)\n",
    "\n",
    "    #Joining this new data and filling NAs with 0\n",
    "    X_train = pd.merge(X_train, cc_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "    X_test = pd.merge(X_test, cc_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "    if validation_set:\n",
    "        X_val = pd.merge(X_val, cc_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "        return X_train, X_test, X_val\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24cdfe3",
   "metadata": {},
   "source": [
    "### 3.7 Using installment payments information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "37c7bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install = pd.read_csv(\"Data/installments_payments.csv\",sep=\",\")\n",
    "\n",
    "# install.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1fd812c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Converting the DAYS columns into positive values\n",
    "# install[\"DAYS_INSTALMENT\"] = install[\"DAYS_INSTALMENT\"].apply(lambda x: abs(x))\n",
    "# install[\"DAYS_ENTRY_PAYMENT\"] = install[\"DAYS_ENTRY_PAYMENT\"].apply(lambda x: abs(x))\n",
    "\n",
    "# #Calculating simple differences\n",
    "# install[\"DAYS_DELAY\"] = install[\"DAYS_ENTRY_PAYMENT\"] - install[\"DAYS_INSTALMENT\"]\n",
    "# install['PAID_OVER_AMOUNT'] = install['AMT_PAYMENT'] - install['AMT_INSTALMENT']\n",
    "\n",
    "# #PAID_OVER flag\n",
    "# install['PAID_OVER'] = install['PAID_OVER_AMOUNT'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# #Difference between DPD and Days before due\n",
    "# install['DPD_diff'] = install['DAYS_ENTRY_PAYMENT'] - install['DAYS_INSTALMENT']\n",
    "# install['DBD_diff'] = install['DAYS_INSTALMENT'] - install['DAYS_ENTRY_PAYMENT']\n",
    "\n",
    "# #Late payment ratio\n",
    "# install['LATE_PAYMENT'] = install.apply(lambda x: 1 if x['DPD_diff'] > 0 else 0, axis=1)\n",
    "# install['INSTALMENT_PAYMENT_RATIO'] = install['AMT_PAYMENT'] / install['AMT_INSTALMENT']\n",
    "# install['LATE_PAYMENT_RATIO'] = install.apply(lambda x: x['INSTALMENT_PAYMENT_RATIO'] if x['LATE_PAYMENT'] == 1 else 0, axis=1)\n",
    "\n",
    "# #Over 100 flag\n",
    "# install['INS_IS_DPD_OVER_100'] = install['DPD_diff'].apply(lambda x: 1 if (x >= 100) else 0)\n",
    "\n",
    "# #We have both high negative and positive delay values which indicate early or very late payment\n",
    "# #We will now calculate the difference in percentage between AMT_INSTALMENT and AMT_PAYMENT\n",
    "# install[\"DEFICIT_PCT\"] = (install[\"AMT_INSTALMENT\"] - install[\"AMT_PAYMENT\"])*100/install[\"AMT_INSTALMENT\"]\n",
    "\n",
    "# install.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7481b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating aggregator\n",
    "# agg_functions = ['min', 'max', 'mean', 'std', 'sum']\n",
    "\n",
    "# #Aggregating on all columns except sk_id_prev\n",
    "# aggregations = {\n",
    "#     c: agg_functions for c in install.drop(columns={\"SK_ID_PREV\",\"SK_ID_CURR\"}).columns\n",
    "# }\n",
    "\n",
    "# #Adding a nunique count on SK_ID_PREV\n",
    "# aggregations[\"SK_ID_PREV\"] = \"nunique\"\n",
    "\n",
    "# #Aggregating over \"SK_ID_CURR\"\n",
    "# install_df = install.groupby(\"SK_ID_CURR\").agg(aggregations).fillna(0)\n",
    "\n",
    "# #Renaming columns to remove multi indexing\n",
    "# install_df.columns = pd.Index(['CC' + '_' + e[0] + '_' + e[1] for e in install_df.columns])\n",
    "\n",
    "\n",
    "# install_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ab5503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving install_df to a csv to prevent repetitive rerun of the program\n",
    "# install_df.to_csv(\"Data/install_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c518a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_install_info(X_train, X_test, X_val=None, validation_set=False):\n",
    "\n",
    "    install_df.set_index(\"SK_ID_CURR\", inplace=True)\n",
    "    install_df.replace([np.inf, -np.inf], 0, inplace=True) #Removing infinite values\n",
    "\n",
    "    #Joining this new data and filling NAs with 0\n",
    "    X_train = pd.merge(X_train, install_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "    X_test = pd.merge(X_test, install_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "    if validation_set:\n",
    "        X_val = pd.merge(X_val, install_df, how='left', left_index=True, right_index=True).fillna(0)\n",
    "        return X_train, X_test, X_val\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "63539904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_csv_preload():\n",
    "    prev_app_df = pd.read_csv(\"Data/prev_app_df.csv\")\n",
    "    bureau_df = pd.read_csv(\"Data/bureau_df.csv\")\n",
    "    cash_df = pd.read_csv(\"Data/cash_df.csv\")\n",
    "    cc_df = pd.read_csv(\"Data/cc_df.csv\")\n",
    "    install_df = pd.read_csv(\"Data/install_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db19b86",
   "metadata": {},
   "source": [
    "## 4 Resampling our training dataset\n",
    "\n",
    "As we've seen at the beginning of this part, our dataset has a very big imbalance with 92% of rows with the TARGET = 0 and only 8% with the Target variable equal to 1.\n",
    "\n",
    "To reduce this imbalance, we will perform oversampling on our minority class.\n",
    "\n",
    "Of course, **oversampling will only be performed on our train set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98acd0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing imblearn to be able to apply different kinds of oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "def over_sample_train(X_train, y_train, method='ADASYN'):\n",
    "    \n",
    "    if method == 'SMOTE':\n",
    "        #Importing the SMOTE algorithm with default values\n",
    "        sm = SMOTE()\n",
    "\n",
    "        #Generating our resampled dataset\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "    \n",
    "    elif method == 'SVMSMOTE':\n",
    "        svm = SVMSMOTE()\n",
    "        #Generating our resampled dataset\n",
    "        X_train_res, y_train_res = svm.fit_resample(X_train, y_train)\n",
    "        \n",
    "    else:\n",
    "        ada = ADASYN()\n",
    "        #Generating our resampled dataset\n",
    "        X_train_res, y_train_res = ada.fit_resample(X_train, y_train)\n",
    "    \n",
    "    return X_train_res, y_train_res, X_train, y_train\n",
    "        \n",
    "\n",
    "# print(X_train_res.shape)\n",
    "# print(y_train_res.value_counts())\n",
    "# #We have successfully removed the imbalance from our dataset and equalized the number of observations for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cdd5eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Renaming the resampled variables for ease of use\n",
    "# X_train_initial = df.copy()\n",
    "# y_train_initial = y_train\n",
    "\n",
    "# X_train = X_train_res.copy()\n",
    "# y_train = y_train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0dce2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Deleting some variables to clear memory\n",
    "# import sys\n",
    "# def sizeof_fmt(num, suffix='B'):\n",
    "#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "#         if abs(num) < 1024.0:\n",
    "#             return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "#         num /= 1024.0\n",
    "#     return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "#                          key= lambda x: -x[1])[:20]:\n",
    "#     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
    "\n",
    "# del df, prev_app, bureau_balance, cash, install, cc, bureau, X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf55fe3",
   "metadata": {},
   "source": [
    "Now that we have resampled our dataset, we want to perform **feature selection** to reduce the number of features and prevent overfitting.\n",
    "\n",
    "## 5. Feature Selection\n",
    "\n",
    "### 5.1 Removing low variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1e2d2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature selection using a variance threshold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def perform_variance_selection(X_train, X_test, X_val=None, validation_set=False, threshold=(0.02)):\n",
    "    \n",
    "    initial_cols = X_train.columns\n",
    "    #We select 2% as our variance threshold, but this is a hyperparameter that we will be able to optimize later\n",
    "    sel = VarianceThreshold(threshold=threshold)\n",
    "    sel.fit(X_train)\n",
    "    print(\"Initial shape:\", X_train.shape)\n",
    "    #Using our selector to remove columns from our 3 sets\n",
    "    X_train_sel = sel.transform(X_train)\n",
    "    X_test_sel = sel.transform(X_test)\n",
    "    if validation_set:\n",
    "        X_val_sel = sel.transform(X_val)\n",
    "\n",
    "    #Creating a list of encoded columns to preserve their names\n",
    "    i = 0\n",
    "    #Retrieving the boolean values for each column (is the column kept or not)\n",
    "    boolean_cols = sel.get_support()\n",
    "    encoded_cols = []\n",
    "    for i in range(len(initial_cols)):\n",
    "        if boolean_cols[i] == True:\n",
    "            encoded_cols.append(initial_cols[i])\n",
    "        i += 1\n",
    "\n",
    "    #The selector has transformed our dataframes into np array, let's turn them back into a DataFrame\n",
    "    X_train = pd.DataFrame(X_train_sel, columns=encoded_cols)\n",
    "    X_test = pd.DataFrame(X_test_sel, columns=encoded_cols)\n",
    "    \n",
    "    if validation_set:\n",
    "        X_val = pd.DataFrame(X_val_sel, columns=encoded_cols)\n",
    "        return X_train, X_test, X_val\n",
    "        \n",
    "    print(\"Final shape: \",X_train.shape)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f331561",
   "metadata": {},
   "source": [
    "### 5.2 Removing highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0fd2a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list features that are correlated\n",
    "# Adds the first of the correlated pair only (not both)\n",
    "def correlatedFeatures(dataset, threshold):\n",
    "    correlated_columns = set()\n",
    "    correlations = dataset.corr()\n",
    "    for i in range(len(correlations)):\n",
    "        for j in range(i):\n",
    "            if abs(correlations.iloc[i,j]) > threshold:\n",
    "                correlated_columns.add(correlations.columns[i])\n",
    "    return correlated_columns\n",
    "\n",
    "# cf = correlatedFeatures(X_train, 0.85)\n",
    "# cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dbaeaabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Removing our highly correlated features\n",
    "# X_train = X_train.drop(cf, axis=1)\n",
    "# X_test = X_test.drop(cf, axis=1)\n",
    "# X_val = X_val.drop(cf, axis=1)\n",
    "\n",
    "# print(X_train.shape, X_test.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9b2e62",
   "metadata": {},
   "source": [
    "### 5.3 Selecting best features\n",
    "\n",
    "We will now use the Kbest algorithm to select the X best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cdff92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "def perform_kbest_selection(df, k, X_train, y_train):\n",
    "\n",
    "    kbest = SelectKBest(score_func=f_regression, k=k)\n",
    "    kbest.fit(X_train, y_train)\n",
    "    print(\"Initial shape:\", df.shape)\n",
    "    df = kbest_transform(df)\n",
    "    print(\"Kbest selection has been performed to select the\",k,\"best features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cac17d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_sel = kbest.transform(X_train)\n",
    "# X_val_sel = kbest.transform(X_val)\n",
    "# X_test_sel = kbest.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd7a8a",
   "metadata": {},
   "source": [
    "Since we are **mostly interested in precision** (we do not want to avoid bad borrowers at any cost, since we still need to make money by allowing the largest part possible of good lenders), we shoud not remove features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973771d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. Model training\n",
    "\n",
    "The client asked for an **easy to interpret** model with an **indication of the importance of each variable** in determining the probability outcome. \n",
    "\n",
    "We will thus compare and tune the 2 following models:\n",
    "\n",
    "- A **Logistic Regression model**\n",
    "- A **Decision Tree Classifier**\n",
    "\n",
    "### 6.1 Selecting a Performance Metric\n",
    "\n",
    "Our task is to try to detect as many \"bad borrowers\" as possible while avoiding false negatives and losing too many clients.\n",
    "\n",
    "My interpretation is that we want to strike the best balance between **precision and recall**, so we will use the **ROC AUC SCORE** as the performance metric for this project.\n",
    "\n",
    "### 6.2 Calculating a Baseline\n",
    "\n",
    "For this project, the baseline will be a model that predicts that **we can lend money to all borrowers** (TARGET = 0).\n",
    "\n",
    "Let's calculate the ROC AUC, accuracy and precision for such a model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "679ce1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    282686\n",
       "1     24825\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the dataset \n",
    "df = pd.read_csv(\"Data/application_train.csv\", sep=\",\")\n",
    "\n",
    "df[\"TARGET\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5ec35988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC : 0.5\n",
      "Accuracy: 0.9192711805431351\n",
      "Precision: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Calculating ROC AUC\n",
    "from sklearn.metrics import roc_auc_score, precision_score, accuracy_score\n",
    "y = df[\"TARGET\"]\n",
    "X = df.drop(columns={\"TARGET\"})\n",
    "#Creating y_predict with the same size as y but only negative values\n",
    "y_predict = pd.DataFrame(y)\n",
    "y_predict[\"TARGET\"] = 0\n",
    "y_predict = y_predict.squeeze()\n",
    "\n",
    "print(\"ROC :\",roc_auc_score(y, y_predict))\n",
    "print(\"Accuracy:\", accuracy_score(y, y_predict))\n",
    "print(\"Precision:\",precision_score(y, y_predict, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668209fa",
   "metadata": {},
   "source": [
    "As we can see, our model has a ROC of 0.5 and a very high accuracy of 92%, but our precision is 0 since it doesn't detect any potentially \"bad\" borrowers.\n",
    "\n",
    "### 6.3 Preprocessing Pipeline\n",
    "\n",
    "We will create the preprocessing pipeline below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a80d7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_csv_preload() #Loading external csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ab79ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding cat vars (246008, 130) (61502, 130)\n",
      "Starting loading external data (246008, 250) (61502, 250)\n",
      "Loaded Previous app (246008, 294) (61502, 294)\n",
      "Loaded CB info (246008, 410) (61502, 410)\n",
      "Loaded Cash info (246008, 441) (61502, 441)\n",
      "Loaded Credit Card info (246008, 548) (61502, 548)\n",
      "Loaded installments info (246008, 629) (61502, 629)\n",
      "Initial shape: (457265, 629)\n",
      "Final shape:  (457265, 392)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/application_train.csv\")\n",
    "#Preprocessing\n",
    "preprocess_cat_vars(df)\n",
    "df = preprocess_num_vars(df)\n",
    "#drop_na_columns(df, 10) #Does not drop any column\n",
    "\n",
    "#Performing train-test split\n",
    "X_train, y_train, X_test, y_test = perform_split(df, target_col='TARGET',\n",
    "                                                 validation_set=False, shuffle=True, random_state=8, test_size=0.2)\n",
    "\n",
    "#Performing data imputation\n",
    "numeric_data_imputation(X_train)\n",
    "numeric_data_imputation(X_test)\n",
    "\n",
    "#Encoding variables\n",
    "encode_cyclical_vars(X_test)\n",
    "encode_cyclical_vars(X_train)\n",
    "print(\"Encoding cat vars\",X_train.shape, X_test.shape)\n",
    "X_test = encode_cat_vars(X_test, X_train, y_train, max_categ=10)\n",
    "X_train = encode_cat_vars(X_train, X_train, y_train, max_categ=10)\n",
    "# X_train = pd.get_dummies(X_train)\n",
    "# X_test = pd.get_dummies(X_test)\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1)\n",
    "\n",
    "print(\"Starting loading external data\",X_train.shape, X_test.shape)\n",
    "#Loading information from external csv files\n",
    "X_train, X_test = load_prev_app_info(X_train, X_test)\n",
    "print(\"Loaded Previous app\",X_train.shape, X_test.shape)\n",
    "X_train, X_test = load_bureau_info(X_train, X_test)\n",
    "print(\"Loaded CB info\",X_train.shape, X_test.shape)\n",
    "X_train, X_test = load_cash_info(X_train, X_test)\n",
    "print(\"Loaded Cash info\",X_train.shape, X_test.shape)\n",
    "X_train, X_test = load_cc_info(X_train, X_test)\n",
    "print(\"Loaded Credit Card info\",X_train.shape, X_test.shape)\n",
    "X_train, X_test = load_install_info(X_train, X_test)\n",
    "print(\"Loaded installments info\",X_train.shape, X_test.shape)\n",
    "\n",
    "#Oversampling\n",
    "X_train, y_train, X_train_init, y_train_init = over_sample_train(X_train, y_train)\n",
    "\n",
    "#Feature selection\n",
    "X_train, X_test = perform_variance_selection(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "202c9332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression results\n",
      "TRAIN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95    226149\n",
      "           1       0.99      0.91      0.95    231116\n",
      "\n",
      "    accuracy                           0.95    457265\n",
      "   macro avg       0.96      0.95      0.95    457265\n",
      "weighted avg       0.96      0.95      0.95    457265\n",
      "\n",
      "ROC AUC train : 0.95\n",
      "----------------------\n",
      "TEST:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.96     56537\n",
      "           1       0.37      0.04      0.07      4965\n",
      "\n",
      "    accuracy                           0.92     61502\n",
      "   macro avg       0.65      0.52      0.51     61502\n",
      "weighted avg       0.88      0.92      0.89     61502\n",
      "\n",
      "ROC AUC train : 0.52\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "#We need to scale the dataset before applying Logistic Regression because sklearn log_r includes L2 regularization\n",
    "pipe_lr = Pipeline([('scaler', StandardScaler()), \n",
    "                    ('log_r', LogisticRegression(max_iter = 3000))])\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "train_predictions = pipe_lr.predict(X_train)\n",
    "test_predictions = pipe_lr.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression results\")\n",
    "print(\"TRAIN:\")\n",
    "print(\"ROC AUC train : {:.5f}\".format(roc_auc_score(y_train, train_predictions)))\n",
    "print(\"----------------------\")\n",
    "print(\"TEST:\")\n",
    "print(classification_report(y_test, test_predictions))\n",
    "print(confusion_matrix(y_test, test_predictions)\n",
    "print(\"ROC AUC test : {:.5f}\".format(roc_auc_score(y_test, test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1e9d8e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56196,   341],\n",
       "       [ 4766,   199]], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77511c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Looking at the best number of features for Logistic Regression\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import precision_score\n",
    "\n",
    "# scores = []\n",
    "# for k in range(1,99):\n",
    "#     #We need to scale the dataset before applying Logistic Regression because sklearn log_r includes L2 regularization\n",
    "#     kbest = SelectKBest(score_func=f_regression, k=k)\n",
    "#     pipe_lr = Pipeline([('scaler', StandardScaler()), ('kbest', kbest), ('log_r', LogisticRegression(max_iter = 1000))])\n",
    "    \n",
    "#     pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "#     train_predictions = pipe_lr.predict(X_train)\n",
    "#     val_predictions = pipe_lr.predict(X_val)\n",
    "#     roc_auc_train = roc_auc_score(y_train, train_predictions)\n",
    "#     roc_auc_val = roc_auc_score(y_val, val_predictions)\n",
    "#     mean_roc = (roc_auc_train + roc_auc_val)/2\n",
    "#     preci_val = precision_score(y_val, val_predictions)\n",
    "    \n",
    "#     scores.append({'k': k, 'roc_train': roc_auc_train, 'roc_val': roc_auc_val,\n",
    "#                    'mean_roc': mean_roc, 'precision': preci_val})\n",
    "\n",
    "# scores = pd.DataFrame(scores)\n",
    "\n",
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b552bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lineplot(data=scores, x='k', y='roc_train', color='green')\n",
    "# sns.lineplot(data=scores, x='k', y='roc_val', color='blue')\n",
    "# ax = sns.lineplot(data=scores, x='k', y='mean_roc', color='red')\n",
    "# ax.set_xlim(left=0, right=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fca66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lineplot(data=scores, x='k', y='precision', color='red')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7efb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c71583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22a380ae",
   "metadata": {},
   "source": [
    "### 6.2 Support Vector Classification (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ceadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pipe_lsvc = Pipeline([('scaler', StandardScaler()), ('svc_l', LinearSVC())])\n",
    "\n",
    "pipe_lsvc.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = pipe_lsvc.predict(X_train)\n",
    "test_predictions = pipe_lsvc.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression results\")\n",
    "print(\"TRAIN:\")\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"----------------------\")\n",
    "print(\"TEST:\")\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31155b24",
   "metadata": {},
   "source": [
    "### 6.3 KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipe_knc = Pipeline([('scaler', StandardScaler()), ('knc', KNeighborsClassifier())])\n",
    "\n",
    "pipe_knc.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = pipe_knc.predict(X_train)\n",
    "test_predictions = pipe_knc.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression results\")\n",
    "print(\"TRAIN:\")\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"----------------------\")\n",
    "print(\"TEST:\")\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1f790",
   "metadata": {},
   "source": [
    "### 6.4 Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#pipe_svc = Pipeline([('scaler', StandardScaler()), ('svc', SVC(verbose=2))])\n",
    "\n",
    "#pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "#train_predictions = pipe_svc.predict(X_train)\n",
    "#test_predictions = pipe_svc.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression results\")\n",
    "print(\"TRAIN:\")\n",
    "#print(classification_report(y_train, train_predictions))\n",
    "print(\"----------------------\")\n",
    "print(\"TEST:\")\n",
    "#print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2639c5",
   "metadata": {},
   "source": [
    "### 6.5 Ensemble Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc128883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "pipe_gbc = Pipeline([('scaler', StandardScaler()), ('EGBC', GradientBoostingClassifier(verbose=3))])\n",
    "\n",
    "pipe_gbc.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = pipe_gbc.predict(X_train)\n",
    "test_predictions = pipe_gbc.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression results\")\n",
    "print(\"TRAIN:\")\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"----------------------\")\n",
    "print(\"TEST:\")\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:project_4]",
   "language": "python",
   "name": "conda-env-project_4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "569.6px",
    "left": "24px",
    "top": "172.125px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
